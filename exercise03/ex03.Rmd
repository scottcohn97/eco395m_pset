---
title: "Exercise 03"
author: "Scott Cohn"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE,
                      warning = FALSE, 
                      message = FALSE)

options(scipen = 999) 

library(tidyverse)
library(ggthemes)
library(tictoc)
library(kableExtra)
library(modelsummary)
library(skimr)
library(estimatr)
library(janitor)
library(tidymodels)
library(patchwork)
library(ggmap)
library(maps)
library(mapdata)
library(vip)
library(rpart.plot)
library(baguette)
```

```{r}
# funcs
read_data <- function(df) {
  #' read data from git url
  #' INPUT: data set name
  #' OUTPUT: dataframe
  full_path <- paste("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/", 
                     df, sep = "")
  df <- read_csv(full_path)
  return(df)
}
```


## What Causes What?

> *Listen to [this podcast](https://www.npr.org/transcripts/178635250) and answer the following.*

### Q1

> *Why can’t I just get data from a few different cities and run the regression of “Crime” on “Police” to understand how more cops in the streets affect crime? (“Crime” refers to some measure of crime rate and “Police” measures the number of cops in a city.)*

*Will hiring more cops cause crime to go down?* This is a causal question. There are feedback effects, too. Cities with high crime may have an incentive to hire more cops. Cities with low crime may have fewer cops. However, the direction could go the other way. Either way not all cities are "equal". There are many confounding factors, so in the end you would have to control for city either way. And, the change in police is not random. This makes causal identification messy.

### Q2

>*How were the researchers from UPenn able to isolate this effect? Briefly describe their approach and discuss their result in the “Table 2” below, from the researchers' paper.*

|                       | (1)     | (2)       |
|-----------------------|---------|-----------|
| High Alert            | -7.316* | -6.046*   |
|                       | (2.877) | (2.537)   |
| Log(midday ridership) |         | 17.341*** |
|                       |         | (5.309)   |
| R-sq                  | 0.14    | 0.17      |

The researchers wanted to find a way to get a lot of police unrelated to crime. The researchers found the terrorism alert system. When the alert system goes to orange, there are extra police put on the Mall and other parts of Washington. This is independent of street crime. They found extra police is associated with lower levels of murder, robbery, and other street crimes. So looking at model (1), we see that when the system is on "high alert" we see crime go down. (The units are unclear from the table alone.) I comment on model (2) in the next question.

### Q3 

> *Why did they have to control for Metro ridership? What was that trying to capture?*

 It's possible that there were less people and tourists on the street due to the alert. The researchers checked for this by looking at ridership levels on the Metro and found that the ridership was largely unchanged. Adding ridership (well `Log(midday ridership)`) to model (2), we see that controlling for ridership diminishes the effect of the high alert slightly, but the effect remains significant.

### Q4

>*Below I am showing you "Table 4" from the researchers' paper. Just focus on the first column of the table. Can you describe the model being estimated here? What is the conclusion?*

|                              | Coefficient |
|------------------------------|-------------|
| High Alert x District 1      | -2.621**    |
|                              | (.044)      |
| High Alert x Other Districts | -.571       |
|                              | (.455)      |
| Log(midday ridership)        | 2.477*      |
|                              | (.364)      |
| Constant                     | -11.058**   |
|                              | (4.211)     |

The model being estimated here would look something like the following:

$$\texttt{Crime} = \beta_0 + \beta_1 \left(\texttt{High_Alert}\times \texttt{D}_1\right) + \beta_2 \left(\texttt{High_Alert}\times \texttt{D}_0\right) + \beta_3 \log(\texttt{midday_ridership}) + \varepsilon,$$

where $\texttt{D}_1$ represents District 1 and $\texttt{D}_0$ represents Other Districts. When there is a high alert, crime in District 1 go down by 2.6, controlling for ridership. High alert in other districts is associated with a small and not significant decline in crime. Since police presence in the "high alert" is localized around District 1, there is evidence that increasing police presence in DC decreases crime.

## Predictive model building: Green Certification

In this problem, we exploit data on 7,894 commercial rental properties from across the United States to build a predictive model for *revenue per square foot per calendar year*, and to use this model to quantify the average change in rental income per square foot (whether in absolute or percentage terms) associated with green certification, holding other features of the building constant.

```{r green_housing_import}
green_build <- 
  read_data("greenbuildings.csv") %>%
  # to snake case
  janitor::clean_names() %>%
  # Revenue per sq ft
  mutate(rev_sqft = rent * leasing_rate) 

# preview data
# green_build %>% 
#   select(-contains("id")) %>%  # remove ID and irrelevant variables
#   modify_if(is.character, as.factor) %>%  # convert character vars to factors
#   skim()  %>% 
#   select(-starts_with("numeric.p")) # remove quartiles
```

We first can look at some plots and short tables.

```{r distro_rev_sqft}
# distro rev_sq_ft
green_build %>%
  ggplot() + 
  geom_histogram(aes(x = rev_sqft), 
                 bins = 50,
                 fill = "dodgerblue", color = "black") +
  geom_vline(aes(xintercept = mean(rev_sqft)), 
             color = "tomato", linetype = 2) +
  # few outliers
  xlim(0, 12000) +
  labs(x = "Revenue by square foot (USD)", y = "Count") + 
  theme_clean() +
  facet_wrap(. ~ green_rating,
             labeller = labeller(green_rating = c("0" = "Not rated", "1" = "Green Rated")))
```

```{r tally_by_group}
# tally by group (green rating)
green_build %>%
  mutate(green_rating = if_else(green_rating == 0, "No rating", "Green rating")) %>%
  group_by(green_rating) %>%
  tally() %>%
  rename(`Green Rating` = green_rating,
         Count = n) %>% 
  kable("pipe")
```

```{r distro_class_a}
# Distro of revenue by class A
green_build %>%
  mutate(class_a = if_else(class_a == 0, "Other", "Class A")) %>%
  ggplot() + 
  geom_histogram(aes(x = rev_sqft, fill = factor(class_a)), 
                 bins = 50,
                 color = "black") +
  geom_vline(aes(xintercept = mean(rev_sqft)), 
             color = "green", linetype = 2) +
  # few outliers
  xlim(0, 12000) +
  labs(x = "Revenue by square foot (USD)", y = "Count",
       caption = "Note: Green = mean") + 
  scale_fill_brewer(palette = "Set1") +
  theme_clean() +
  theme(legend.title = element_blank())
```

Now we can split into our train/test split and validate the samples with cross-fold validation. We use these splits and folds across several models.

```{r splits_resamples}
# Split into train/test split
set.seed(395)
green_split <- initial_split(green_build, strata = rev_sqft)
green_train <- training(green_split)
green_test <- testing(green_split)

# v-fold
set.seed(3951)
green_folds <- vfold_cv(green_train, v = 3, strata = rev_sqft)
green_folds
```

Next, these data require feature engineering, so we exploit the tidymodels recipe format to make these steps easily executable across multiple models.

```{r feat_eng}
green_rec <- 
  recipe(rev_sqft ~ ., green_train) %>% 
  update_role(contains("id"), new_role = "ID") %>% # declare ID variables
  step_mutate(
    green_cert = case_when(
      leed == 1 ~ "leed",
      energystar == 1 ~ "energystar",
      leed == 0 & energystar == 0 ~ "none"
    )
  ) %>%
  step_rm(c(rent, leasing_rate, leed, energystar, green_rating, total_dd_07, city_market_rent)) %>% # remove confounders
  #step_bin2factor(c(green_rating)) %>% # binary to factor
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove variables with zero variances
  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels 
  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("ID"))  %>% # replaces missing numeric observations with the median
  step_dummy(all_nominal(), -has_role("ID")) # dummy codes categorical variables

```

Then, I have created three models: a decision tree, a KNN-regression, and a LASSO regression. 

### Decision Tree

First, we specify a model with tuneable hyperparameters and then create a grid of tuning parameters. 

```{r tree_spec_grid}
# create tuneable dt model
tree_spec <- decision_tree(
  cost_complexity = tune(), # 0 > cp > tune()
  tree_depth = tune(),      # max tree depth
  min_n = tune()            # smallest node allowed
) %>%
  set_engine("rpart") %>%   # set tree engine
  set_mode("regression")

tree_spec

# create grids of tuning params
tree_grid <- 
  grid_regular(
    cost_complexity(), 
    tree_depth(), 
    min_n(), 
    levels = 4
    )

tree_grid
```

A model workflow allows me to rerun parts of the model easily.

```{r}
# create workflow
wflow_tree <- 
  workflow() %>% 
  add_recipe(green_rec) %>%
  add_model(tree_spec)
```

Using the workflow, the hyperparameter tuning can be done with ease. We run it in parallel to speed up computation time. Then we visualize specified model metrics against tree depth.

```{r}
# try all param values on resampled datasets
doParallel::registerDoParallel()

set.seed(3452)

tree_rs <- 
  tune_grid(
    wflow_tree,
    resamples = green_folds,
    grid = tree_grid, 
    metrics = metric_set(rmse, rsq, mae)
  )

tree_rs

# evaluate model
# collect_metrics(tree_rs)

autoplot(tree_rs) + theme_clean()
```

To apply this model to the test set, we select the model with the lowest RMSE and fit the model to the test set.

```{r final_tree_fit}
lowest_tree_rmse <- select_best(tree_rs, "rmse")

## Out of sample performance

# finalize workflow
final_wf <- 
  wflow_tree %>% 
  finalize_workflow(lowest_tree_rmse)

final_wf

# fit the model (two ways, same thing)
final_rs <- last_fit(final_wf, green_split)

# tree_rs %>%
#   show_best(metric = "rmse")
```

```{r}
# look at test data
collect_metrics(final_rs)[,1:3] %>% kbl(digits = 3, format = "pipe")

# what are the most important variables
# final_wf %>% 
#   pull_workflow_fit() %>% 
#   vip(geom = "col", aesthetics = list(fill = "midnightblue", alpha = 0.8)) +
#   scale_y_continuous(expand = c(0, 0)) +
#   theme_clean()

# look at predictions
final_rs %>%
  collect_predictions() %>%
  ggplot(aes(rev_sqft, .pred)) +
  geom_abline(slope = 1, lty = 2, color = "gray50", alpha = 0.5) +
  geom_point(alpha = 0.6, color = "midnightblue") +
  theme_clean() +
  coord_fixed()
```

### KNN-regression

Similar to previous, I define a model engine with a tuneable K.

```{r}
# set model with tuneable hyperparams
knn_spec <-
  nearest_neighbor(
    mode = "regression",
    neighbors = tune("K")
  ) %>%
  set_engine("kknn")

# construct workflow
wflow_knn <-
  workflow() %>% 
  add_recipe(green_rec) %>%
  add_model(knn_spec)

knn_set <-
  parameters(wflow_knn) %>%
  # try k in 1:50
  update(K = neighbors(c(1, 50)))

set.seed(3952)
knn_grid <-
  knn_set %>%
  grid_max_entropy(size = 50)

knn_grid_search <-
  tune_grid(
    wflow_knn,
    resamples = green_folds,
    grid = knn_grid
  )
```

After tuning the model, I select the best performing across the number of neighbors and look at the out of sample performance.

```{r best_knn}
# choose best model
lowest_rmse_knn <- select_best(knn_grid_search, "rmse")

best_k <- as.numeric(lowest_rmse_knn$K)

# plot rmse x K
collect_metrics(knn_grid_search) %>%
  filter(.metric == "rmse") %>% 
  ggplot() + 
  geom_vline(aes(xintercept = best_k), linetype = 2, color = "tomato") +
  geom_point(aes(x = K, y = mean), color = "midnightblue", alpha = 0.8) +
  labs(y = "RMSE") +
  theme_clean()
```

```{r knn_final}
## Out of sample performance

# finalize workflow
final_wf_knn <- 
  wflow_knn %>% 
  finalize_workflow(lowest_rmse_knn)

final_wf_knn

# fit the model 
last_fit(
  final_wf_knn,
  green_split
  ) %>%
  collect_metrics() %>% 
  select(1:3) %>%
  kbl(digits = 3, format = "pipe")
```

### LASSO

Here, I construct a penalized regression, or LASSO model, with a tuneable penalty.

```{r lasso_spec}
lasso_spec <- 
  linear_reg(
    penalty = tune(), 
    mixture = 1
    ) %>%
  set_engine("glmnet")

# construct workflow
wflow_lasso <-
  workflow() %>% 
  add_recipe(green_rec) %>%
  add_model(lasso_spec)

lambda_grid <- grid_regular(penalty(), levels = 50)
```

```{r tune_grid_lasso}
doParallel::registerDoParallel()
set.seed(3955)
lasso_grid <- 
  tune_grid(
    wflow_lasso,
    resamples = green_folds,
    grid = lambda_grid
  )
```

```{r lasso_metrics, eval=FALSE, include=FALSE}
lasso_grid %>%
  collect_metrics() %>%
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```

```{r lowest_rmse_lasso}
lowest_rmse <- 
  lasso_grid %>%
  select_best("rmse")

final_lasso <- 
  finalize_workflow(
    wflow_lasso,
    lowest_rmse
  )

final_lasso %>%
  fit(green_train) %>%
  pull_workflow_fit() %>%
  vi(lambda = lowest_rmse$penalty) %>%
  mutate(
    Importance = abs(Importance),
    Variable = fct_reorder(Variable, Importance)
  ) %>%
  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
  geom_col() +
  scale_fill_brewer(palette = "Set1") +
  scale_x_continuous(expand = c(0, 0)) +
  labs(y = NULL) +
  theme_clean()
```

```{r lasso_lastfit}
last_fit(
  final_lasso,
  green_split
  ) %>%
  collect_metrics() %>% 
  select(1:3) %>%
  kbl(digits = 3, format = "pipe")
```

## Predictive model building: California housing

In this problem, we exploit census-tract level data on residential housing in California to build a predictive model for `medianHouseValue`.

```{r ca_housing_import}
housing <- 
  read_data("CAhousing.csv") %>%
  janitor::clean_names()
```

Thankfully Professor Scott made this easier for us and eliminated all of the missing values problems. Nice work there.

```{r cali_map}
# cali <- ggmap(ggmap::get_stamenmap(location ='California', zoom = 6))
# ggmap(cali)

states <- map_data("state")
county <- map_data("county")
ca_df <- subset(states, region == "california")
ca_county <- subset(county, region == "california")

ca_base <- 
  ggplot() + 
  coord_fixed(1.3) + 
  geom_polygon(data = ca_df, 
               aes(x = long, y = lat, group = group),
               color = "black", fill = "white")  + 
  geom_polygon(data = ca_county, 
               aes(x = long, y = lat, group = group), 
               fill = NA, color = "gray") +
  geom_polygon(data = ca_df,
               aes(x = long, y = lat, group = group), 
               color = "black", fill = NA)

ca_base + 
  geom_point(data = housing, 
    aes(x = longitude, y = latitude, 
        color = median_house_value, size = population), 
    alpha = 0.4) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_map() +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(title = "California Housing",
       x = "Longitude", y = "Latitude",
       color = "Median House Value (in $USD)", 
       size = "Population")
```

### Cleaning

The data is pretty clean, so there isn't much feature engineering to do.

Here we create the train/test split.

```{r}
set.seed(395)

# Create a split object
housing_split <- initial_split(housing, prop = 0.75, strata = median_house_value)

# Build training data set
housing_train <- housing_split %>% training()

# Build testing data set
housing_test <- housing_split %>% testing()

# vfold
housing_vfold <- vfold_cv(housing_train, v = 10, strata = median_house_value)
```

First, we can fit a basic linear model as a baseline. I do this as I've done in previous problem sets using the `tidymodels` pipeline. 

```{r}
set.seed(395)

# specify a linear model
lm_model <- 
  linear_reg() %>% 
  set_engine('lm') %>% 
  set_mode('regression')

# define a recipe - FEATURE ENG
lm_recipe <- 
  # fit on all variables
  recipe(median_house_value ~ ., data = housing_train) %>%
  # log price
  step_log(median_house_value) %>%
  # standardize
  step_range(total_bedrooms, total_rooms, population, housing_median_age, median_income) %>%
  # specify tuning hyperparameters
  step_ns(longitude, deg_free = tune("long df")) %>% 
  step_ns(latitude,  deg_free = tune("lat df"))

# grid to tun long/lat
grid_vals <- seq(2, 22, by = 2)
# A regular grid:
spline_grid <- expand.grid(`long df` = grid_vals, `lat df` = grid_vals)

# which hyper param to tune
housing_param <- 
  lm_recipe %>% 
  parameters() %>% 
  update(
    `long df` = spline_degree(), 
    `lat df` = spline_degree()
  )

housing_param

# create a workflow
lm_workflow <- 
  workflow() %>% 
  # specify engine
  add_model(lm_model) %>% 
  # specify recipe
  add_recipe(lm_recipe)

tic()
lm_res <- 
  lm_workflow %>%
  tune_grid(resamples = housing_vfold, grid = spline_grid)
toc()

lm_est <- collect_metrics(lm_res)

lm_rmse_vals <- 
  lm_est %>% 
  dplyr::filter(.metric == "rmse") %>% 
  arrange(mean)

lm_final <-
  lm_rmse_vals %>%
  filter(.metric == "rmse") %>%
  filter(mean == min(mean))


lm_final_workflow <- 
  lm_workflow %>% 
  finalize_workflow(lm_final)

# fit the model
lm_fit <- 
  # use the workflow with the best model ...
  lm_final_workflow %>% 
  # ... to fit the test set
  last_fit(split = housing_split)

# Obtain performance metrics on test data
lm_fit %>% collect_metrics()
```

Let's plot spline functions. Looking at these plots, the smaller degrees of freedom (red) are clearly under-fitting. Visually, the more complex splines (blue) might indicate that there is overfitting but this would result in poor RMSE values when computed on the hold-out data.

```{r}
housing_train %>% 
  dplyr::select(median_house_value, longitude, latitude) %>% 
  tidyr::pivot_longer(cols = c(longitude, latitude), 
                      names_to = "predictor", values_to = "value") %>% 
  ggplot(aes(x = value, median_house_value)) + 
  geom_point(alpha = .2) + 
  geom_smooth(se = FALSE, method = lm, formula = y ~ splines::ns(x, df = 3),  col = "tomato") +
  geom_smooth(se = FALSE, method = lm, formula = y ~ splines::ns(x, df = 16)) +
  scale_y_log10() +
  theme_clean() +
  facet_wrap(~ predictor, scales = "free_x")
```

Let's save predictions on the out-of-sample test set.

```{r}
# Obtain test set predictions data frame
lm_results <- 
  lm_fit %>% 
  # save pred results
  collect_predictions()

# bind prediction columns
lm_results <- 
  lm_results %>% 
  bind_cols(housing_test) %>% 
  rename(median_house_value_log = `median_house_value...4`,
         median_house_value = `median_house_value...14`) 
```

Visually, how did we do?

```{r}
# plot pred v actual
lm_results %>%
  ggplot(aes(x = .pred, y = median_house_value_log)) +
  geom_point(color = '#006EA1', alpha = 0.25)  +
  geom_abline(intercept = 0, slope = 1, color = 'tomato') +
  labs(title = 'Linear Regression Results - Test Set',
       x = 'Predicted Price',
       y = 'Actual Price') + 
  theme_clean()
```

This looks okay. Let's recreate the first map, but now looking at predicted values. Not that the predicted values are log-price.

```{r}
p1 <- 
  ca_base + 
  geom_point(data = lm_results, 
    aes(x = longitude, y = latitude, 
        color = .pred, size = population), 
    alpha = 0.4) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_map() +
  scale_color_distiller(palette = "Spectral", labels = comma,
                        limits = c(9, 14)) +
  labs(title = "Predicted ",
       x = "Longitude", y = "Latitude",
       color = "Median House Value (in $USD)", 
       size = "Population")

p2 <- 
  ca_base + 
  geom_point(data = lm_results, 
    aes(x = longitude, y = latitude, 
        color = median_house_value_log, size = population), 
    alpha = 0.4) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_map() +
  scale_color_distiller(palette = "Spectral", labels = comma,
                        limits = c(9, 14)) +
  labs(title = "Actual",
       x = "Longitude", y = "Latitude",
       color = "Median House Value (in $USD)", 
       size = "Population")

p1 + p2 + 
  plot_layout(guides = 'collect') +
  plot_annotation(title = "California Housing -- Linear Model")
```

To see more concretely where this model was less accurate, we can plot the error against the longitude and latitude. Here, green-to-blue means the model guessed too high of a price and orange-to-red suggests the model underestimated. It seems this model did quite well given the amount fo yellow.

```{r}
ca_base + 
  geom_point(data = lm_results, 
    aes(x = longitude, y = latitude, 
        color = median_house_value_log - .pred, size = population), 
    alpha = 0.4) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_map() +
  scale_color_distiller(palette = "Spectral", labels = comma) +
  labs(title = "California Housing, log error",
       x = "Longitude", y = "Latitude",
       color = "Median House Value (in $USD)", 
       size = "Population")
```

How does this compare to a KNN-regression? A KNN-regression might give better performance with respect to the non-linearities in our features.

```{r}
set.seed(395)

# specify a knn model
knn_model <- 
  # specify hyperparameters
  nearest_neighbor(neighbors = tune(), weight_func = tune()) %>% 
  set_engine('kknn') %>% 
  set_mode('regression') %>%
  translate()

# define a recipe
knn_recipe <- 
  # fit on all variables
  recipe(median_house_value ~ ., data = housing_train) %>%
  # log price
  step_log(median_house_value) %>%
  # standardize
  step_range(total_bedrooms, total_rooms, population, housing_median_age, median_income) %>%
  # specify tuning hyperparameters
  step_ns(longitude, deg_free = tune("long df")) %>% 
  step_ns(latitude,  deg_free = tune("lat df"))

# create a workflow
knn_workflow <- 
  workflow() %>% 
  # specify engine
  add_model(knn_model) %>% 
  # specify recipe
  add_recipe(knn_recipe)
```

After feature engineering and specifying the model, we tune the hyperparameter

```{r}
knn_param <- 
  knn_workflow %>% 
  # how to tune hyperparams
  parameters() %>% 
    update(
    `long df` = spline_degree(c(2, 18)), 
    `lat df` = spline_degree(c(2, 18)),
    neighbors = neighbors(c(3, 50)),
    weight_func = weight_func(values = c("rectangular", "inv", "triangular"))
  )

ctrl <- control_bayes(verbose = TRUE)

tic()
knn_search <- 
  tune_bayes(knn_workflow, resamples = housing_vfold, initial = 5, iter = 10,
                         param_info = knn_param, control = ctrl)
toc()

knn_final <-
  knn_search %>%
  collect_metrics() %>% 
  dplyr::filter(.metric == "rmse") %>% 
  filter(mean == min(mean))


knn_final_workflow <- 
  knn_workflow %>% 
  finalize_workflow(knn_final)

# fit the model
knn_fit <- 
  # use the workflow with the best model ...
  knn_final_workflow %>% 
  # ... to fit the test set
  last_fit(split = housing_split)

# Obtain performance metrics on test data
knn_fit %>% 
  collect_metrics() %>%
  select(1:3) %>%
  kbl(digits = 3, format = "pipe")
```

We were able to get some gains over the linear model. Here is the final model used:

```{r}
knn_final_workflow
```

Let's save predictions on the out-of-sample test set.

```{r}
# Obtain test set predictions data frame
knn_results <- 
  knn_fit %>% 
  # save pred results
  collect_predictions()
```

Visually, how did we do?

```{r}
# plot pred v actual
knn_results %>%
  ggplot(aes(x = .pred, y = median_house_value)) +
  geom_point(color = '#006EA1', alpha = 0.25)  +
  geom_abline(intercept = 0, slope = 1, color = 'tomato') +
  labs(title = 'KNN Regression Results - Test Set',
       x = 'Predicted Price',
       y = 'Actual Price') + 
  theme_clean() 
```

From the RMSE alone, this model is a bit better. To improve, we may have to do some more feature engineering or use a different modeling framework altogether.

Since this is the better model of the two, let's look at the predicted (log) residual as a deviation away from the true value:

```{r}
# then use this map
knn_results <- 
  knn_results %>%
    bind_cols(housing_test) %>% 
    rename(median_house_value_log = `median_house_value...4`,
           median_house_value = `median_house_value...14`) 

knn_results %>% 
  arrange(median_house_value_log) %>%
  mutate(id = row_number()) %>%
  ggplot(aes(x = id, y = median_house_value_log)) + 
  geom_segment(aes(xend = id, yend = .pred), alpha = .2) +
  geom_point(aes(y = .pred), shape = 1) + 
  geom_point(color = "tomato", shape = 1, alpha = 0.5) +
  labs(x = "", y = "Logged median house value") + 
  theme_clean()
```

This model is fairly accurate, and the gains over the linear model are marginal. The data set contains enough unique features that predicted house value is not terribly difficult. The challenge is standardizing and weighting variables appropriately, such as I have done with spline-tuning the the longitude and latitude. I think a simpler model controlling for census block or a binary `coastal` variable would be equally effective. I imagine it is a strong indicator of price.

## Session Information 

```{r sysinfo}
sessionInfo()
```


