---
title: "Exercise 04"
author: "Scott Cohn, with Olalekan Bello and Colin Wick"
date: "Last compiled on `r format(Sys.time(), '%d %B, %Y')`"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      cache = TRUE,
                      warning = FALSE, 
                      message = FALSE)

options(scipen = 999) 

library(tidyverse)
library(ggthemes)
library(tictoc)
library(kableExtra)
library(modelsummary)
library(skimr)
library(estimatr)
library(janitor)
library(tidymodels)
library(patchwork)
library(arules)
library(arulesViz)
library(tm)
library(wordcloud)
library(SnowballC)
library(LICORS)
library(mosaic)
library(pander)
library(viridis)
library(stargazer)
library(ggcorrplot)
library(readtext)
library(e1071)
library(gmodels)
```

```{r}
# funcs
read_data <- function(df) {
  #' read data from git url
  #' INPUT: data set name
  #' OUTPUT: dataframe
  full_path <- paste("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/", 
                     df, sep = "")
  df <- read_csv(full_path)
  return(df)
}
```

## Clustering and PCA

We use k-means clustering  as our choice of clustering algorithm. We use two clusters. The table shows the averages for the features within the two clusters.

Cluster 1
```{r}
set.seed(1234)

wine <- read_data("wine.csv")
X = wine[,-(12:13)]
X = scale(X, center = TRUE, scale=TRUE)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")


# Run k-means with 2 clusters and 50 starts
clust1 = kmeanspp(X, 2, nstart=50)

clust1$center[1,]*sigma + mu
```

Cluster 2 
```{r}
clust1$center[2,]*sigma + mu
```


To check whether the clustering algorithm is appropriately able to identify the colors, we check the means for the features grouped by color. They look very similar to the means calculated from our raw data and it looks like cluster 1 is red wine while cluster 2 is white wine. 


```{r}
table1 <- 
  wine %>%
  group_by(color) %>%
  summarise(across(
    .cols = where(is.numeric), 
    .fns = list(Mean = mean), na.rm = TRUE, 
    .names = "{col}_{fn}"
    ))

pander(table1,style = "grid")
```

Let's get a confusion matrix.

```{r}
wine <- wine %>% mutate(truth= ifelse(color == "red", 2, 1))

confusion= table(truth=wine$truth, clust_pred= clust1$cluster)
acc <- round(sum(diag(confusion)/sum(confusion))*100, 2)
confusion
print(paste("Our clustering algorithm has accuracy of ", acc, "%"), quote= F)
```

Let's now apply clustering to see if we can identify quality. Below is a heatmap of the correlations between all the properties. We see that quality is not particularly strongly related with any one chemical property. Density looks to be the most negatively correlated so we'll use that going forward.

We use k-means clustering again. We use three clusters as we might want to think about the quality in terms of low, high and medium. 

```{r}
# looks a mess -- reorder the variables by hierarchical clustering
ggcorrplot::ggcorrplot(cor(wine[, c(1:12)]), hc.order = TRUE)

# Now run hierarchical clustering
cluster1 = kmeanspp(X, 3, nstart=50)
```

The figure below plots density and quality by cluster. We do not see any particularly clear patterns emerge in term of quality as there looks to be a fair mix of different quality wines in all the clusters.However, we notice that cluster 1 could possibly have properties of high quality wine as we see less of cluster 2 and 3 making up a smaller proportion of the wines between the ranges of 7-9 

```{r}
qplot(wine$density, wine$quality, color= factor(cluster1$cluster), xlab = "Density", ylab= "Quality")+theme_bw()
```

We now apply PCA and use the first two components. Figure 2 below is a plot of the components colored by the wine color.

```{r}
pc_wine = prcomp(X)
scores  = pc_wine$x
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2') + theme_bw()
```
It looks like component 1 is able to identify the wine color pretty well. We see that the whites are clustered more towards the right and reds are clustered to the left.

\newpage 
Below is a graph of our two main principal components colored by quality.

```{r, fig.width=11, fig.height=7}
qplot(scores[,1], scores[,2], color= wine$quality, xlab='Component 1', ylab='Component 2') + scale_color_viridis() + theme_bw()
```

We see that while it's not particularly perfect and not as clear as the distinction for color, the components are also able to identify quality at some level. The higher quality wines look to be mainly around the bottom right. Indicating that component 1 weighs positively on quality while component 2 weighs negatively on it. To confirm, we regress quality on components 1 and 2 and the sign of our coefficients confirm this and we see that they are also statistically significant. 

```{r, results= 'asis'}
reg1 <- lm(formula = wine$quality ~ scores[, 1] + scores[, 2])
stargazer(reg1, header = F, omit.stat = c("adj.rsq", "F"), covariate.labels = c("PC1", "PC2"), type = "html")
```

## Market Segmentation

```{r}
sm <- read_data("social_marketing.csv")
sm %>% select(-X1,-chatter) %>%
  colSums() %>% data.frame() %>% arrange(desc(`.`)) %>% head(10) %>%
  rownames_to_column() %>%
  rename("Topic" = 1,"Count"=2) %>%
  kableExtra::kbl("pipe")
top10 <- sm %>% select(-X1,-chatter) %>%
  colSums() %>% data.frame() %>% arrange(desc(`.`)) %>% head(10) %>% rownames()
```

From here we see the beginnings of the market structure just by looking at aggregate mentions of each topic, but this is not a detailed marketing strategy. 

```{r include=FALSE}
sm_net <- sm %>%
  pivot_longer(-X1) %>%
  mutate(edge = ifelse(value != 0,1,0)) %>%
  filter(edge != 0 & value > 5)%>% unique() %>% filter(name != "chatter") %>%
  select(name,X1) %>% data.frame()
sm_net_list = split(x = sm_net$name,f=sm_net$X) 
sm_net_list <- as(sm_net_list,"transactions")
topic_assoc = apriori(sm_net_list, 
	parameter=list(support=.01, confidence=.5, maxlen=2),appearance = )
```

```{r}
plot(head(topic_assoc, 25, by='lift'), method="graph" )
```

We removed the "chatter" category because everyone chatters now and then. Instead, we look at the likelihood of people to talk about a topic based on talking about another. The visualization above creates groups based on people's likely topics of conversation. Though most of these seem intuitive, the clear topic groupings. 

The size of each node represents the amount of times it appeared in the data, meaning a higher share of the audience consistently talked about the topic. From this we see multiple clear segments; 

1. An outdoorsy, personal health and fitness type consumer, likely to be marketed to via signage and retail placement.

2. An online gaming college student consumer, likely to be marketed to on Twitch and other gaming-oriented social media sites.

3. A beauty, cooking, fashion, and photo sharing consumer, likely marketed to on Instagram.

4. Politics, news, travel, and automotive interested consumer, likely marketed to on Facebook and Twitter.

5. The food, religion, parenting consumer, who may be more difficult to reach in a measured way due to the low relative concentration on any particular digital media. Grocery stores and traditional media would be likely places to build your brand.

```{r}
sm1 <- sm %>%
  column_to_rownames("X1")
sm_scl <- sm1 %>% scale(center=T,scale = T) 
  mu = attr(sm_scl,"scaled:center")
  sigma = attr(sm_scl,"scaled:scale")
clust1 <- kmeanspp(sm_scl,k = 5)
sm1$Total = rowSums(sm1)
sm1$cluster <- clust1$cluster
```

```{r}
sm1 %>% group_by(cluster) %>% summarize(colSums(sm1))
colSums(sm1[sm1$cluster==1,])
clusters <- data.frame(matrix(nrow=5,ncol=length(names(sm1))))
clusters[1,] <- colSums(sm1[sm1$cluster==1,])
clusters[2,] <- colSums(sm1[sm1$cluster==2,])
clusters[3,] <- colSums(sm1[sm1$cluster==3,])
clusters[4,] <- colSums(sm1[sm1$cluster==4,])
clusters[5,] <- colSums(sm1[sm1$cluster==5,])
names(clusters) <- names(sm1)
cluster_vars <- data.frame(matrix(nrow=5,ncol=5))
cluster_vars[,1] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X1)) %>% head(5) %>% row.names()
cluster_vars[,2] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X2)) %>% head(5) %>% row.names()
cluster_vars[,3] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X3)) %>% head(5) %>% row.names()
cluster_vars[,4] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X4)) %>% head(5) %>% row.names()
cluster_vars[,5] <- clusters %>% select(-Total,-chatter) %>% mutate(cluster = c(1:5)) %>% t() %>% data.frame() %>% arrange(desc(X5)) %>% head(5) %>% row.names()
cluster_vars %>% kableExtra::kbl("pipe") 
```

## Association Rules for Grocery Purchases

First we load the data.

```{r}
data("Groceries")
```

We can examine the sparse matrix of transactions and items in a simple plot.

```{r}
image(sample(Groceries, 100))
dev.off() 
```

What are the most frequent item purchases?

```{r}
frequent_items <- eclat(Groceries, parameter = list(supp = 0.07, maxlen = 15)) 

summary(frequent_items)
```

Next, we use the `apriori` function, which implements the Apriori algorithm to mine frequent itemsets, to define rules for purchasing associations.  

```{r}
rules <- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.8, maxlen = 3)) # Min Support as 0.001, confidence as 0.8.

# remove redundant rules (not needed)
# subset_matrix <- is.subset(rules, rules)
# subset_matrix[lower.tri(subset_matrix, diag = T)] <- NA # not working
# redundant <- colSums(subset_matrix, na.rm = T) >= 1
# rules_pruned <- rules[!redundant]
# rules <- rules_pruned

summary(rules)
```

Some of the rules can be visualized.

```{r}
plot(rules, engine = "ggplot") + theme_clean()
plot(rules, "scatterplot", engine = "ggplot") + theme_clean()
plot(rules, "grouped", engine = "default") 
head(quality(rules)) %>% kbl(digits = 4, "pipe")

arules::itemFrequencyPlot(
  Groceries,
  topN = 15,
  col = 'dodgerblue',
  main = 'Relative Item Frequency Plot',
  type = "relative",
  ylab = "Item Frequency"
  )

plot(rules, method = "graph", control = list(type = "items"), engine = "igraph")

plot(rules, method = "paracoord", control = list(type = "items"))
```

From the visualizations and the summaries of item pairs, I recommend the following aisles:

1. Groceries Aisle – Milk, Eggs and Vegetables
2. Liquor Aisle – Liquor, Red/Blush Wine, Bottled Beer, Soda
3. Eateries Aisle – Herbs, Tropical Fruits, Rolls/Buns, Fruit Juices, Jams
4. Breakfast Aisle – Cereals, Yogurt, Rice, Curd

## Author Attribution

First, we load the data.

```{r}
## Collect data

# training data
Data_train <- readtext(Sys.glob('../../ECO395M/data/ReutersC50/C50train/*'))
# head(Data_train$text, n = 1)

# testing data
Data_test <- readtext(Sys.glob('../../ECO395M/data/ReutersC50/C50test/*'))
```

Then we pull author names from the file directory and assign them to texts, and do a check to make sure it worked as expected.

```{r}
# author names
author_names <- as.data.frame(rep(basename(list.dirs('../../ECO395M/data/ReutersC50/C50train')), each = 50))
author_names <- author_names[-(1:50),]

# assign author name to Text
Data_test$author <- author_names
Data_train$author <- author_names

# dropping ID column
Data_test <- Data_test[-1]
Data_train <- Data_train[-1]

# converting author column to factor
Data_test$author <- as.factor(Data_test$author)
Data_train$author <- as.factor(Data_train$author)

# did it work?
table(Data_train$author) %>% kbl("pipe")
```

Next, we create the corpus. This is split into a train/test and are stripped of punctuation, forced to lowercase, and numbers are removed --- as well as whitespace and common stopwords. I use simple wordclouds to check if this process is working as expected.

```{r}
## Explore and Prep

# Create corpus
test_corpus <- Corpus(VectorSource(Data_test$text))
train_corpus <- Corpus(VectorSource(Data_train$text))

# clean corpus
test_corpus <-
  test_corpus %>%
  tm_map(., content_transformer(tolower)) %>%
  tm_map(., content_transformer(removeNumbers)) %>%
  tm_map(., content_transformer(removePunctuation)) %>%
  tm_map(., content_transformer(stripWhitespace)) %>%
  tm_map(., content_transformer(removeWords), stopwords("SMART"))

# did it work?
# inspect(test_corpus[1])
wordcloud(test_corpus, min.freq = 40, random.order = FALSE)

train_corpus <-
  train_corpus %>%
  tm_map(., content_transformer(tolower)) %>%
  tm_map(., content_transformer(removeNumbers)) %>%
  tm_map(., content_transformer(removePunctuation)) %>%
  tm_map(., content_transformer(stripWhitespace)) %>%
  tm_map(., content_transformer(removeWords), stopwords("SMART"))
```

To analyze the text, I create document-term matrices from the corpuses.

```{r}
# document term matrix (sparse matrices)
test_dtm <- DocumentTermMatrix(test_corpus)
train_dtm <- DocumentTermMatrix(train_corpus)

# inspect(train_dtm)
```

Finally, with the document-term matrices, I use a naive-bayes classifier to predict the author of the text using a dictionary of words unique to each article.

```{r}
## Naive Bayes Classification
freq_words <- findFreqTerms(train_dtm, 5)

# saving List using Dictionary() Function
Dictionary <- function(x) {
  if (is.character(x)) {
    return(x)
  }
  stop('x is not a character vector')
}

data_dict <- Dictionary(findFreqTerms(train_dtm, 5))

# appending Document Term Matrix to Train and Test Dataset 
data_train <- DocumentTermMatrix(train_corpus, list(data_dict))
data_test <- DocumentTermMatrix(test_corpus, list(data_dict))

# converting the frequency of word to count
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0)
  x <- factor(x, levels = c(0, 1), labels = c("No", "Yes")) 
  return(x)
}

# appending count function to Train and Test Dataset
data_train <- apply(data_train, MARGIN = 2, convert_counts)
data_test <- apply(data_test, MARGIN = 2, convert_counts)

# train model
data_classifier <- naiveBayes(data_train, Data_train$author)

data_test_pred <- predict(data_classifier, data_test)
# CrossTable(data_test_pred, Data_test$author,
#            prop.chisq = FALSE, prop.t = FALSE,
#            dnn = c('predicted', 'actual'))
```

I apply the trained model to the test set and compare the "actual author" to the predicted author. 

```{r}
final_df <- 
  tibble(
    "predicted" = data_test_pred,
    "actual" = Data_test$author
  )

num_correct <- 
  final_df %>% 
  mutate(correct = if_else(predicted == actual, 1, 0)) %>%
  pull(correct) %>%
  sum()

num_rows <- final_df %>% nrow()

num_correct / num_rows
```

This model guesses correctly (out of 50 authors) 70\% of the time. 

## Session Information 

```{r sysinfo}
sessionInfo()
```

